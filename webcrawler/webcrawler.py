import sys

import requests
from bs4 import BeautifulSoup

TC = []
C = set()


def request(url):
    header = {"User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0"}
    try:
        r = requests.get(url, headers=header)
        return r.text
    except KeyboardInterrupt:
        sys.exit(0)
    except:
        pass
        sys.exit(0)


def get_l(html):
    links = []
    try:
        soup = BeautifulSoup(html, "html.parser")
        refs = soup.find_all("a", href=True)
        for ref in refs:
            link = ref["href"]
            if link.startswith("http"):
                links.append(link)

        return links
    except:
        pass


def crawl():
    try:
        while 1:
            if TC:
                url = TC.pop()

                html = request(url)
                if html:
                    links = get_l(html)
                    if links:
                        for link in links:
                            if link not in C and link not in TC:
                                TC.append(link)

                    print("[ + ] Found: {}".format(url))

                    C.add(url)
                else:
                    C.add(url)
            else:
                print("Done")
                break
    except KeyboardInterrupt:
        print("\n Keyboard Interrupt")
    except:
        pass


if __name__ == "__main__":
    if len(sys.argv) <= 1:
        print("")
        print("===============================")
        print("##         WEBCRAWLER        ##")
        print("===============================")
        print("")
        print("Use: python webcrawler.py <url>")
        sys.exit(0)
    url = sys.argv[1]
    TC.append(url)
    crawl()
